{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32b2a27",
   "metadata": {},
   "source": [
    "# Gu√≠a de Instalaci√≥n y Pr√°ctica ‚Äî Sesi√≥n 1 (Notebook)\n",
    "Curso: **IA en el Aula ‚Äî Nivel Avanzado**  \n",
    "Profesor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **22 de octubre de 2025**\n",
    "\n",
    "Este cuaderno gu√≠a a los participantes para configurar el entorno, conectarse a una API de modelos de lenguaje y realizar las primeras consultas con **par√°metros clave** como `temperature`, `max_tokens`, `top_p`, etc. Se incluyen explicaciones paso a paso y ejercicios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0ddd",
   "metadata": {},
   "source": [
    "## 1) Requisitos previos\n",
    "- Python 3.10 o superior\n",
    "- Cuenta y **clave API** en un proveedor de modelos de lenguaje (ej. OpenAI)\n",
    "- Editor/entorno: VSCode, Jupyter o Colab\n",
    "- Conexi√≥n a Internet\n",
    "\n",
    "Si est√°s en **Colab**, puedes ejecutar el siguiente comando para instalar dependencias. En local, usa tu terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc702d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\usuario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# (Opcional en local/Colab) Instalar dependencias\n",
    "%pip install --quiet openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e55a",
   "metadata": {},
   "source": [
    "## 2) Preparar variables de entorno\n",
    "Crea un archivo `.env` en la carpeta del proyecto con:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=tu_clave_aqui\n",
    "```\n",
    "Nunca publiques tu clave. Evita subir `.env` a repositorios p√∫blicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80b7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente inicializado. Modelo listo para consultas.\n"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Lee el archivo .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00221859",
   "metadata": {},
   "source": [
    "Si est√° en Google Colab, debe incluir la llave en los secretos (en el men√∫ de la izquierda), activar la variable y luego inicializar as√≠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd38611",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3) Cargar la clave y crear el cliente en google Colab\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[0;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39muserdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCliente inicializado. Modelo listo para consultas.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente en google Colab\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174eb5a6",
   "metadata": {},
   "source": [
    "## 4) Par√°metros importantes de la API (explicaci√≥n breve)\n",
    "- **`model`**: identifica el modelo a consultar (p.ej., `gpt-4o-mini`).\n",
    "- **`messages`**: lista de turnos conversacionales. Usa roles `system` (instrucciones generales), `user` (tu prompt), `assistant` (respuestas previas si las hay).\n",
    "- **`temperature`**: *aleatoriedad* de la salida. Valores bajos (0.0‚Äì0.3) hacen respuestas m√°s deterministas; valores altos (0.7‚Äì1.0) hacen respuestas m√°s creativas.\n",
    "- **`top_p`**: alternativa a `temperature` basada en muestreo por probabilidad acumulada; usa uno u otro (no ambos a la vez a valores lejanos) para afinar el estilo.\n",
    "- **`max_tokens`**: tope de tokens generados en la respuesta. Si es muy bajo, el texto puede cortarse.\n",
    "- **`stop`**: secuencias de corte; si aparecen, la generaci√≥n se detiene.\n",
    "- **`frequency_penalty` / `presence_penalty`**: penalizaciones para reducir repeticiones y fomentar aparici√≥n de nuevos t√©rminos.\n",
    "\n",
    "**Buenas pr√°cticas:**\n",
    "- Controlar `temperature` (0.2‚Äì0.7) seg√∫n la tarea (evaluaci√≥n ‚Üí bajo; lluvia de ideas ‚Üí medio/alto).\n",
    "- Estructurar prompts y, cuando sea √∫til, **pedir JSON** para facilitar la automatizaci√≥n.\n",
    "- Evitar incluir datos personales o sensibles en prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873e7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial en la educaci√≥n se refiere al uso de tecnolog√≠as avanzadas para personalizar y mejorar el proceso de ense√±anza y aprendizaje, adaptando recursos y metodolog√≠as a las necesidades individuales de los estudiantes. Esto incluye herramientas como tutores virtuales, an√°lisis de datos para evaluar el rendimiento acad√©mico y sistemas que facilitan la administraci√≥n educativa.\n"
     ]
    }
   ],
   "source": [
    "# 5) Primera consulta: respuesta libre\n",
    "prompt = \"Explica en dos frases qu√© es la inteligencia artificial en la educaci√≥n.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7  # m√°s creativo que 0.2, menos que 1.0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e281741",
   "metadata": {},
   "source": [
    "### Comentarios sobre `temperature`\n",
    "- A `0.0‚Äì0.2`: respuestas casi siempre iguales (√∫til en **rubricas** o **explicaciones est√°ndar**).\n",
    "- A `0.5‚Äì0.8`: m√°s variaci√≥n l√©xica/estil√≠stica (√∫til en **generaci√≥n de materiales** o **brainstorming**).\n",
    "- A `>0.9`: creatividad alta pero riesgo de salidas menos precisas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f22c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"operation\": \"explanation\",\n",
      "  \"input\": \"¬øQu√© es aprendizaje supervisado?\",\n",
      "  \"output\": \"El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza com√∫nmente en tareas como clasificaci√≥n y regresi√≥n.\"\n",
      "}\n",
      "\n",
      "Valid JSON ‚Üí {'operation': 'explanation', 'input': '¬øQu√© es aprendizaje supervisado?', 'output': 'El aprendizaje supervisado es un tipo de aprendizaje autom√°tico donde un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza com√∫nmente en tareas como clasificaci√≥n y regresi√≥n.'}\n"
     ]
    }
   ],
   "source": [
    "# 6) Respuesta estructurada en JSON para automatizaci√≥n\n",
    "import json\n",
    "\n",
    "query = \"¬øQu√© es aprendizaje supervisado?\"\n",
    "schema_instruction = (\n",
    "    \"Responde en formato JSON con las claves: operation, input, output. \"\n",
    "    \"operation debe ser 'explanation'; input debe repetir la pregunta; output la explicaci√≥n clara y breve.\"\n",
    ")\n",
    "response_json = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": schema_instruction},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ],\n",
    "    temperature=0.1,       # m√°s determinista para formatos estructurados\n",
    "    max_tokens=300         # suficiente para una explicaci√≥n breve\n",
    ")\n",
    "text = response_json.choices[0].message.content\n",
    "print(text)\n",
    "\n",
    "# (Opcional) intentar cargar como JSON si el modelo devolvi√≥ un objeto v√°lido\n",
    "try:\n",
    "    data = json.loads(text)\n",
    "    print(\"\\nValid JSON ‚Üí\", data)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nLa salida no es JSON v√°lido literal. Puedes parsearla manualmente o usar validadores/funciones JSON del proveedor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a1d82",
   "metadata": {},
   "source": [
    "## 7) Ejercicios propuestos\n",
    "1. Cambia `temperature` a 0.1, 0.5 y 0.9 y compara el estilo de las respuestas.\n",
    "2. Pide que el modelo responda **siempre en JSON** usando un `system` que lo exija. Verifica si cumple.\n",
    "3. Crea un prompt de tu √°rea (p.ej., *programaci√≥n*, *arquitectura*, *matem√°ticas*) que devuelva un JSON con `operation`, `input`, `steps` (lista) y `output`.\n",
    "4. Limita la longitud con `max_tokens` y observa si corta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eece738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"Explica brevemente el principio de funcionamiento de un √°rbol de decisi√≥n.\",\n",
      "  \"respuesta\": \"Un √°rbol de decisi√≥n es un modelo de aprendizaje autom√°tico que utiliza una estructura jer√°rquica para tomar decisiones basadas en preguntas sobre las caracter√≠sticas de los datos. Cada nodo interno representa una prueba sobre un atributo, cada rama representa el resultado de la prueba y cada hoja representa una clase o resultado final, permitiendo clasificar o predecir resultados de manera intuitiva.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Plantilla reutilizable para el curso\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(ask_model(\n",
    "    \"Explica brevemente el principio de funcionamiento de un √°rbol de decisi√≥n.\",\n",
    "    temperature=0.4,\n",
    "    system=\"Responde en dos oraciones, tono docente y preciso. Siempre responde JSON con llaves promt y respuesta\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779287fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo para C√°lculo de Energ√≠a Cin√©tica ---\n",
      "{\n",
      "  \"operation\": \"C√°lculo de Energ√≠a Cin√©tica\",\n",
      "  \"input\": \"m=1500 kg, v=20 m/s\",\n",
      "  \"steps\": [\n",
      "    \"La f√≥rmula de la energ√≠a cin√©tica es: EK = (1/2) * m * v^2\",\n",
      "    \"Sustituyendo los valores: EK = (1/2) * 1500 kg * (20 m/s)^2\",\n",
      "    \"Calculando: EK = (1/2) * 1500 kg * 400 m^2/s^2\",\n",
      "    \"EK = 750 kg * 400 m^2/s^2\",\n",
      "    \"EK = 300000 kg*m^2/s^2\",\n",
      "    \"EK = 300000 J\"\n",
      "  ],\n",
      "  \"output\": \"300000 J\"\n",
      "}\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os # Importar para obtener la clave API, si se usa un entorno\n",
    "\n",
    "# ‚ö†Ô∏è ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Este es un ejemplo de c√≥mo podr√≠as inicializarlo (ajusta seg√∫n tu configuraci√≥n):\n",
    "# Si usas variables de entorno:\n",
    "# client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# üí° Para prop√≥sitos de este ejemplo, si no tienes la clave configurada \n",
    "# en el entorno, deber√°s inicializar 'client' o comentar esta l√≠nea si \n",
    "# la ejecuci√≥n real ocurre en un entorno donde 'client' ya est√° definido.\n",
    "# Si est√°s ejecutando en un Jupyter/Colab con la clave ya definida, no es necesario.\n",
    "# Si la variable 'client' no est√° definida, el c√≥digo fallar√°.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ‚ö†Ô∏è Esta funci√≥n requiere que la variable 'client' est√© definida globalmente\n",
    "    # y que la conexi√≥n a la API sea exitosa.\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except NameError:\n",
    "        return \"ERROR: La variable 'client' (OpenAI client) no est√° definida. Por favor, inicial√≠zala.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (F√≠sica) ---\n",
    "\n",
    "# üéØ PROMPT DE F√çSICA: C√°lculo de Energ√≠a Cin√©tica.\n",
    "prompt_fisica = \"Calcula la energ√≠a cin√©tica de un coche que tiene una masa de 1500 kg y se mueve a una velocidad constante de 20 m/s. Formula la ley f√≠sica utilizada.\"\n",
    "\n",
    "# ü§ñ SYSTEM: Exige el formato JSON estricto.\n",
    "system_json_request = \"\"\"\n",
    "Tu √∫nica tarea es resolver el problema de F√≠sica.\n",
    "DEBES responder √∫nicamente con un objeto JSON que contenga las siguientes claves:\n",
    "1. 'operation': La ley f√≠sica utilizada o el objetivo del c√°lculo (e.g., 'C√°lculo de Energ√≠a Cin√©tica').\n",
    "2. 'input': Los datos iniciales proporcionados (e.g., 'm=1500 kg, v=20 m/s').\n",
    "3. 'steps': Una lista de cadenas, donde cada elemento detalla una etapa del c√°lculo o la formulaci√≥n de la ley. Incluye la f√≥rmula.\n",
    "4. 'output': El resultado final del c√°lculo con unidades (e.g., '300000 J').\n",
    "NO incluyas texto adicional fuera del objeto JSON.\n",
    "\"\"\"\n",
    "\n",
    "# --- Ejemplo de Uso y Ejecuci√≥n ---\n",
    "\n",
    "print(\"--- Ejecutando Modelo para C√°lculo de Energ√≠a Cin√©tica ---\")\n",
    "print(ask_model(\n",
    "    prompt=prompt_fisica,\n",
    "    temperature=0.1, # Baja temperatura para consistencia y precisi√≥n\n",
    "    system=system_json_request\n",
    "))\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d332f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo para C√°lculo de Elasticidad Precio de la Demanda ---\n",
      "{\n",
      "  \"operation\": \"C√°lculo de Elasticidad Precio de la Demanda (EPD) utilizando el m√©todo del punto medio\",\n",
      "  \"input\": \"Pi=$10, Qi=100, Pf=$12, Qf=90\",\n",
      "  \"steps\": [\n",
      "    \"1. Calcular el cambio en la cantidad demandada: ŒîQ = Qf - Qi = 90 - 100 = -10.\",\n",
      "    \"2. Calcular el cambio en el precio: ŒîP = Pf - Pi = 12 - 10 = 2.\",\n",
      "    \"3. Calcular el promedio de la cantidad demandada: Qprom = (Qi + Qf) / 2 = (100 + 90) / 2 = 95.\",\n",
      "    \"4. Calcular el promedio del precio: Pprom = (Pi + Pf) / 2 = (10 + 12) / 2 = 11.\",\n",
      "    \"5. Aplicar la f√≥rmula de la elasticidad: EPD = (ŒîQ / Qprom) / (ŒîP / Pprom) = (-10 / 95) / (2 / 11).\",\n",
      "    \"6. Calcular EPD: EPD = (-0.1053) / (0.1818) = -0.58.\",\n",
      "    \"7. Interpretaci√≥n: La elasticidad precio de la demanda es -0.58, lo que indica que la demanda es inel√°stica.\"\n",
      "  ],\n",
      "  \"output\": {\n",
      "    \"value\": -0.58,\n",
      "    \"type\": \"inel√°stica\"\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json # Importar para una mejor manipulaci√≥n de JSON\n",
    "\n",
    "# ‚ö†Ô∏è ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Si usas la librer√≠a de OpenAI, la variable 'client' debe estar definida \n",
    "# con tu clave API para que el c√≥digo se ejecute correctamente.\n",
    "# Ejemplo (ajusta seg√∫n tu entorno):\n",
    "# client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ‚ö†Ô∏è Esta funci√≥n requiere que la variable 'client' est√© definida.\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Simulaci√≥n de la respuesta para entornos sin la clave API configurada\n",
    "        # Si 'client' est√° definido y la clave es v√°lida, se ejecutar√° la llamada real.\n",
    "        if 'client' not in globals() and 'client' not in locals():\n",
    "            print(\"AVISO: 'client' no est√° definido. Devolviendo una respuesta de ejemplo simulada.\")\n",
    "            return json.dumps({\n",
    "                \"operation\": \"C√°lculo de Elasticidad Precio de la Demanda (EPD)\",\n",
    "                \"input\": \"Precio inicial ($10), Cantidad inicial (100 unidades), Precio final ($12), Cantidad final (90 unidades)\",\n",
    "                \"steps\": [\n",
    "                    \"F√≥rmula de la EPD (M√©todo del Punto Medio): EPD = [(Qf - Qi) / ((Qf + Qi)/2)] / [(Pf - Pi) / ((Pf + Pi)/2)].\",\n",
    "                    \"C√°lculo del cambio porcentual en la cantidad demandada: [(90 - 100) / ((90 + 100)/2)] = -10 / 95 ‚âà -0.1053.\",\n",
    "                    \"C√°lculo del cambio porcentual en el precio: [(12 - 10) / ((12 + 10)/2)] = 2 / 11 ‚âà 0.1818.\",\n",
    "                    \"C√°lculo de la EPD: -0.1053 / 0.1818 ‚âà -0.579.\"\n",
    "                ],\n",
    "                \"output\": \"La Elasticidad Precio de la Demanda es aproximadamente -0.58. La demanda es inel√°stica porque el valor absoluto (|0.58|) es menor que 1.\"\n",
    "            }, indent=2)\n",
    "\n",
    "        # Llamada real a la API (si 'client' est√° definido)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except NameError:\n",
    "        return \"ERROR: La variable 'client' (OpenAI client) no est√° definida. Por favor, inicial√≠zala.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (Econom√≠a) ---\n",
    "\n",
    "# üéØ PROMPT DE ECONOM√çA: C√°lculo de la EPD.\n",
    "prompt_economia = \"Calcula la Elasticidad Precio de la Demanda utilizando el m√©todo del punto medio si el precio de un bien sube de $10 a $12, causando que la cantidad demandada baje de 100 unidades a 90 unidades. Indica qu√© tipo de elasticidad es.\"\n",
    "\n",
    "# ü§ñ SYSTEM: Exige el formato JSON estricto.\n",
    "system_json_request = \"\"\"\n",
    "Tu √∫nica tarea es resolver el problema de Econom√≠a y proporcionar la interpretaci√≥n del resultado.\n",
    "DEBES responder √∫nicamente con un objeto JSON que contenga las siguientes claves:\n",
    "1. 'operation': La ley econ√≥mica utilizada (e.g., 'C√°lculo de Elasticidad Precio de la Demanda (EPD)').\n",
    "2. 'input': Los datos iniciales proporcionados (e.g., 'Pi=$10, Qi=100, Pf=$12, Qf=90').\n",
    "3. 'steps': Una lista de cadenas, donde cada elemento detalla una etapa del c√°lculo, la f√≥rmula y la interpretaci√≥n.\n",
    "4. 'output': El resultado num√©rico final y el tipo de elasticidad.\n",
    "NO incluyas texto adicional, explicaciones ni encabezados fuera del objeto JSON.\n",
    "\"\"\"\n",
    "\n",
    "# --- Ejemplo de Uso y Ejecuci√≥n ---\n",
    "\n",
    "print(\"--- Ejecutando Modelo para C√°lculo de Elasticidad Precio de la Demanda ---\")\n",
    "print(ask_model(\n",
    "    prompt=prompt_economia,\n",
    "    temperature=0.1, # Muy baja temperatura para asegurar precisi√≥n num√©rica y formato\n",
    "    system=system_json_request\n",
    "))\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab2d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Ejecuci√≥n con max_tokens BAJO (40 tokens) ---\n",
      "La fusi√≥n nuclear es un proceso f√≠sico en el cual dos n√∫cleos at√≥micos ligeros se combinan para formar un n√∫cleo m√°s pesado, liberando una cantidad significativa de energ√≠a en el proceso\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- 2. Ejecuci√≥n con max_tokens ALTO (400 tokens) ---\n",
      "La fusi√≥n nuclear es un proceso f√≠sico en el cual dos n√∫cleos at√≥micos ligeros se combinan para formar un n√∫cleo m√°s pesado, liberando una cantidad significativa de energ√≠a en el proceso. Este fen√≥meno es el responsable de la producci√≥n de energ√≠a en las estrellas, incluida nuestra propia estrella, el Sol.\n",
      "\n",
      "### Proceso de Fusi√≥n Nuclear\n",
      "\n",
      "La fusi√≥n nuclear ocurre cuando los n√∫cleos de dos √°tomos ligeros, como el hidr√≥geno, se acercan lo suficiente como para superar la repulsi√≥n electrost√°tica que existe entre ellos debido a sus cargas positivas. Para que esto suceda, es necesario alcanzar condiciones extremas de temperatura y presi√≥n, que permiten que los n√∫cleos se fusionen. En el caso del Sol, estas condiciones se logran en su n√∫cleo, donde las temperaturas alcanzan aproximadamente 15 millones de grados Celsius y las presiones son inmensas.\n",
      "\n",
      "El proceso m√°s com√∫n de fusi√≥n en el Sol es la fusi√≥n de is√≥topos de hidr√≥geno, como el deuterio y el tritio, que se combinan para formar helio, liberando energ√≠a en forma de radiaci√≥n electromagn√©tica y part√≠culas. La reacci√≥n b√°sica puede representarse de la siguiente manera:\n",
      "\n",
      "\\[ ^2H + ^3H \\rightarrow ^4He + n + energ√≠a \\]\n",
      "\n",
      "### Importancia de la Fusi√≥n Nuclear\n",
      "\n",
      "La fusi√≥n nuclear es considerada una fuente de energ√≠a potencialmente revolucionaria por varias razones:\n",
      "\n",
      "1. **Abundancia de Combustible**: Los is√≥topos de hidr√≥geno, como el deuterio, son abundantes en el agua de los oc√©anos, lo que significa que la materia prima para la fusi√≥n es pr√°cticamente inagotable.\n",
      "\n",
      "2. **Energ√≠a Limpia**: A diferencia de la fisi√≥n nuclear, que produce residuos radiactivos de larga vida, la fusi√≥n genera helio, un gas inerte y no contaminante. Adem√°s\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ‚ö†Ô∏è ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Este c√≥digo asume que la variable 'client' est√° definida globalmente\n",
    "# y la conexi√≥n a la API est√° configurada.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400, # Valor por defecto m√°s com√∫n\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para interactuar con un modelo de lenguaje y observar el efecto de max_tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Manejo b√°sico de la ausencia de 'client' para simulaci√≥n\n",
    "    try:\n",
    "        if 'client' not in globals() and 'client' not in locals():\n",
    "            raise NameError(\"La variable 'client' no est√° definida.\")\n",
    "            \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens # üëà El valor que se va a limitar\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except NameError:\n",
    "        # Respuesta simulada si 'client' no est√° definido\n",
    "        simulated_response = (\n",
    "            \"La fusi√≥n nuclear es el proceso mediante el cual dos n√∫cleos at√≥micos ligeros se combinan para formar un n√∫cleo m√°s pesado. Este proceso libera una enorme cantidad de energ√≠a, mucho mayor que la fisi√≥n nuclear. Ocurre naturalmente en el Sol y las estrellas, donde las altas temperaturas y presiones superan la repulsi√≥n coul√≥mbica de los n√∫cleos. En la Tierra, se busca replicar esta reacci√≥n en reactores de confinamiento magn√©tico (como los tokamaks) o inercial, utilizando is√≥topos de hidr√≥geno (deuterio y tritio) como combustible, para crear una fuente de energ√≠a limpia y pr√°cticamente inagotable. Es el santo grial de la energ√≠a.\"\n",
    "        )\n",
    "        if max_tokens < len(simulated_response.split()):\n",
    "            return f\"SIMULACI√ìN DE CORTE (max_tokens={max_tokens}): {simulated_response[:max_tokens*5]}...\" # Aprox. 5 caracteres por token en espa√±ol\n",
    "        return simulated_response\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso ---\n",
    "\n",
    "# üí¨ PROMPT que pide una respuesta LAAAARGA para forzar el corte.\n",
    "prompt_largo = \"Explica detalladamente qu√© es la fusi√≥n nuclear, d√≥nde ocurre, por qu√© es importante y los principales desaf√≠os para su implementaci√≥n en la Tierra. Necesito una explicaci√≥n extensa.\"\n",
    "\n",
    "# ü§ñ SYSTEM: Instrucci√≥n de tono simple.\n",
    "system_instruccion = \"Responde con un tono informativo y acad√©mico.\"\n",
    "\n",
    "\n",
    "# --- Ejemplo de Uso y Verificaci√≥n ---\n",
    "\n",
    "print(\"--- 1. Ejecuci√≥n con max_tokens BAJO (40 tokens) ---\")\n",
    "# El modelo deber√≠a detenerse abruptamente en medio de una frase.\n",
    "print(ask_model(\n",
    "    prompt=prompt_largo,\n",
    "    temperature=0.1, # Baja para que la respuesta sea coherente hasta el corte\n",
    "    max_tokens=40,   # üëà Valor bajo para forzar el corte\n",
    "    system=system_instruccion\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"--- 2. Ejecuci√≥n con max_tokens ALTO (400 tokens) ---\")\n",
    "# El modelo deber√≠a completar toda la explicaci√≥n.\n",
    "print(ask_model(\n",
    "    prompt=prompt_largo,\n",
    "    temperature=0.1, \n",
    "    max_tokens=400, # üëà Valor alto, suficiente para la respuesta\n",
    "    system=system_instruccion\n",
    "))\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b061c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo con L√≠mite de Tokens (max_tokens=40) ---\n",
      "Pregunta: Explica en detalle los principios fundamentales de la arquitectura Von Neumann y por qu√© se la conoce como el 'cuello de botella'.\n",
      "L√≠mite de tokens establecido: 40\n",
      "\n",
      "### RESPUESTA CORTADA ###\n",
      "La arquitectura Von Neumann es un modelo de dise√±o de computadoras que fue propuesto por el matem√°tico y f√≠sico John von Neumann en la d√©cada de 1940. Este modelo ha sido fundamental\n",
      "\n",
      "--- Observaci√≥n ---\n",
      "Como puedes ver, la respuesta se interrumpe abruptamente (usualmente a mitad de una frase) porque el modelo alcanz√≥ el l√≠mite de 40 tokens antes de terminar la explicaci√≥n o de cerrar cualquier estructura (como un JSON o una frase completa).\n",
      "\n",
      "--- Ejecutando Modelo con L√≠mite ALTO (max_tokens=400) para comparaci√≥n ---\n",
      "\n",
      "### RESPUESTA COMPLETA (Simulada o Real) ###\n",
      "La arquitectura Von Neumann es un modelo de dise√±o de computadoras que fue propuesto por el matem√°tico y f√≠sico John von Neumann en la d√©cada de 1940. Este modelo ha sido fundamental en el desarrollo de la computaci√≥n moderna y se basa en una serie de principios que definen c√≥mo se organizan y operan los componentes de un sistema inform√°tico. A continuaci√≥n, se describen en detalle los principios fundamentales de la arquitectura Von Neumann y se explica por qu√© se le conoce como el \"cuello de botella\".\n",
      "\n",
      "### Principios Fundamentales de la Arquitectura Von Neumann\n",
      "\n",
      "1. **Unidad de Procesamiento Central (CPU)**:\n",
      "   - La CPU es el coraz√≥n del sistema y se encarga de ejecutar las instrucciones de los programas. Se compone de dos partes principales: la Unidad Aritm√©tico-L√≥gica (ALU) y la Unidad de Control (CU).\n",
      "   - La ALU realiza operaciones aritm√©ticas y l√≥gicas, mientras que la CU gestiona el flujo de datos entre la CPU y otros componentes del sistema, decodificando las instrucciones y controlando su ejecuci√≥n.\n",
      "\n",
      "2. **Memoria**:\n",
      "   - La arquitectura Von Neumann utiliza un √∫nico espacio de memoria para almacenar tanto los datos como las instrucciones del programa. Esto significa que la memoria es un recurso compartido, lo que permite que la CPU acceda a las instrucciones y a los datos de manera uniforme.\n",
      "   - La memoria se organiza en direcciones, y cada ubicaci√≥n de memoria puede contener un dato o una instrucci√≥n. Esto permite que el sistema sea flexible y que las instrucciones se modifiquen durante la ejecuci√≥n.\n",
      "\n",
      "3. **Bus de Datos**:\n",
      "   - La comunicaci√≥n entre la CPU, la memoria y otros dispositivos perif√©ricos se realiza a trav√©s de un bus de datos. Este bus es un conjunto de l√≠neas de comunicaci√≥n que transportan datos entre los diferentes componentes del sistema.\n",
      "   - El bus de datos es un canal compartido, lo que significa que solo\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os \n",
    "# ‚ö†Ô∏è ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Si est√°s en un entorno donde 'client' no est√° definido, el c√≥digo usar√°\n",
    "# la respuesta simulada para demostrar el corte.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400, # Valor por defecto\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulaci√≥n de la respuesta si 'client' no est√° definido,\n",
    "    # para demostrar el efecto del max_tokens.\n",
    "    if 'client' not in globals() and 'client' not in locals():\n",
    "        # Simulamos una respuesta que deber√≠a ser larga, pero se corta a 40 tokens.\n",
    "        simulated_response = {\n",
    "            \"prompt\": prompt,\n",
    "            \"respuesta_completa_esperada\": \"La arquitectura Von Neumann es un dise√±o fundamental que utiliza una √∫nica memoria para almacenar tanto las instrucciones del programa como los datos que ser√°n procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella...\",\n",
    "            \"respuesta_simulada_cortada\": \"La arquitectura Von Neumann utiliza una √∫nica memoria para almacenar tanto las instrucciones del programa como los datos que ser√°n procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella conocido como el 'cuello de botella de Von Neumann', que limita la velocidad de procesamiento al tener que acceder a una misma ruta para ambas tareas. Esta limitaci√≥n afecta directamente el rendimiento...\",\n",
    "            \"respuesta_cortada_a_40_tokens\": \"La arquitectura Von Neumann utiliza una √∫nica memoria para almacenar tanto las instrucciones del programa como los datos que ser√°n procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella conocido como el 'cuello de botella de Von Neumann', que limita la velocidad de procesamiento al tener que acceder a una misma ruta para\"\n",
    "        }\n",
    "        \n",
    "        # Devolvemos la respuesta cortada simulada para el ejercicio\n",
    "        # N√≥tese que el modelo real NO genera un JSON incompleto, sino que corta el texto.\n",
    "        if max_tokens <= 40:\n",
    "             # Devolvemos el texto cortado para demostrar el efecto\n",
    "             # Ignoramos temporalmente la exigencia de JSON para mostrar el corte puro.\n",
    "            return simulated_response[\"respuesta_cortada_a_40_tokens\"] + \"...\" \n",
    "        else:\n",
    "            return \"Simulaci√≥n de respuesta larga sin corte.\"\n",
    "        \n",
    "    # Si 'client' est√° definido, se ejecuta la llamada real a la API\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens # Aqu√≠ aplicamos el l√≠mite\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: La llamada real fall√≥. {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (Inform√°tica) ---\n",
    "\n",
    "# üéØ PROMPT DE INFORM√ÅTICA: Pide una explicaci√≥n detallada.\n",
    "prompt_informatica = \"Explica en detalle los principios fundamentales de la arquitectura Von Neumann y por qu√© se la conoce como el 'cuello de botella'.\"\n",
    "\n",
    "# ü§ñ SYSTEM: Instrucci√≥n de tono (manteniendo el tono)\n",
    "system_request = \"Responde en un tono t√©cnico y detallado. La respuesta debe ser lo m√°s extensa posible para probar el l√≠mite de tokens.\"\n",
    "\n",
    "# --- Ejemplo de Uso con max_tokens BAJO (40) ---\n",
    "\n",
    "LIMITE_DE_TOKENS = 40\n",
    "\n",
    "print(\"--- Ejecutando Modelo con L√≠mite de Tokens (max_tokens=40) ---\")\n",
    "print(f\"Pregunta: {prompt_informatica}\")\n",
    "print(f\"L√≠mite de tokens establecido: {LIMITE_DE_TOKENS}\\n\")\n",
    "\n",
    "respuesta_cortada = ask_model(\n",
    "    prompt=prompt_informatica,\n",
    "    temperature=0.1,\n",
    "    max_tokens=LIMITE_DE_TOKENS, # üö® Aqu√≠ se aplica el l√≠mite bajo\n",
    "    system=system_request\n",
    ")\n",
    "\n",
    "print(\"### RESPUESTA CORTADA ###\")\n",
    "print(respuesta_cortada)\n",
    "print(\"\\n--- Observaci√≥n ---\")\n",
    "print(\"Como puedes ver, la respuesta se interrumpe abruptamente (usualmente a mitad de una frase) porque el modelo alcanz√≥ el l√≠mite de 40 tokens antes de terminar la explicaci√≥n o de cerrar cualquier estructura (como un JSON o una frase completa).\")\n",
    "\n",
    "# --- Ejemplo de Uso con max_tokens ALTO (400) ---\n",
    "print(\"\\n--- Ejecutando Modelo con L√≠mite ALTO (max_tokens=400) para comparaci√≥n ---\")\n",
    "\n",
    "respuesta_completa = ask_model(\n",
    "    prompt=prompt_informatica,\n",
    "    temperature=0.1,\n",
    "    max_tokens=400,\n",
    "    system=system_request\n",
    ")\n",
    "\n",
    "print(\"\\n### RESPUESTA COMPLETA (Simulada o Real) ###\")\n",
    "print(respuesta_completa)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fef7c",
   "metadata": {},
   "source": [
    "## 9) Soluci√≥n de problemas comunes\n",
    "- **`openai.AuthenticationError` / `401`**: clave inv√°lida o no cargada; revisa tu `.env` y reinicia el kernel.\n",
    "- **`Rate limit`**: excediste el n√∫mero de solicitudes por minuto; espera unos segundos y reintenta.\n",
    "- **`model_not_found`**: el modelo no existe o no tienes acceso; cambia a uno disponible en tu cuenta.\n",
    "- **Salidas no JSON**: fija `temperature=0.0‚Äì0.3`, agrega instrucciones `system` estrictas y/o usa funciones nativas de validaci√≥n JSON si el proveedor las ofrece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eab4f2",
   "metadata": {},
   "source": [
    "## 10) Pr√≥ximos pasos\n",
    "En la siguiente sesi√≥n se ver√°n **consultas avanzadas**, manejo de contexto y el punto de partida para construir un **asistente con RAG** (recuperaci√≥n de conocimiento) usando materiales del curso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
