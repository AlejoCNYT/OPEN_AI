{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32b2a27",
   "metadata": {},
   "source": [
    "# Guía de Instalación y Práctica — Sesión 1 (Notebook)\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Profesor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **22 de octubre de 2025**\n",
    "\n",
    "Este cuaderno guía a los participantes para configurar el entorno, conectarse a una API de modelos de lenguaje y realizar las primeras consultas con **parámetros clave** como `temperature`, `max_tokens`, `top_p`, etc. Se incluyen explicaciones paso a paso y ejercicios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0ddd",
   "metadata": {},
   "source": [
    "## 1) Requisitos previos\n",
    "- Python 3.10 o superior\n",
    "- Cuenta y **clave API** en un proveedor de modelos de lenguaje (ej. OpenAI)\n",
    "- Editor/entorno: VSCode, Jupyter o Colab\n",
    "- Conexión a Internet\n",
    "\n",
    "Si estás en **Colab**, puedes ejecutar el siguiente comando para instalar dependencias. En local, usa tu terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc702d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\usuario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# (Opcional en local/Colab) Instalar dependencias\n",
    "%pip install --quiet openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e55a",
   "metadata": {},
   "source": [
    "## 2) Preparar variables de entorno\n",
    "Crea un archivo `.env` en la carpeta del proyecto con:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=tu_clave_aqui\n",
    "```\n",
    "Nunca publiques tu clave. Evita subir `.env` a repositorios públicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80b7f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente inicializado. Modelo listo para consultas.\n"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  # Lee el archivo .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00221859",
   "metadata": {},
   "source": [
    "Si está en Google Colab, debe incluir la llave en los secretos (en el menú de la izquierda), activar la variable y luego inicializar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd38611",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3) Cargar la clave y crear el cliente en google Colab\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[0;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39muserdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCliente inicializado. Modelo listo para consultas.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# 3) Cargar la clave y crear el cliente en google Colab\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174eb5a6",
   "metadata": {},
   "source": [
    "## 4) Parámetros importantes de la API (explicación breve)\n",
    "- **`model`**: identifica el modelo a consultar (p.ej., `gpt-4o-mini`).\n",
    "- **`messages`**: lista de turnos conversacionales. Usa roles `system` (instrucciones generales), `user` (tu prompt), `assistant` (respuestas previas si las hay).\n",
    "- **`temperature`**: *aleatoriedad* de la salida. Valores bajos (0.0–0.3) hacen respuestas más deterministas; valores altos (0.7–1.0) hacen respuestas más creativas.\n",
    "- **`top_p`**: alternativa a `temperature` basada en muestreo por probabilidad acumulada; usa uno u otro (no ambos a la vez a valores lejanos) para afinar el estilo.\n",
    "- **`max_tokens`**: tope de tokens generados en la respuesta. Si es muy bajo, el texto puede cortarse.\n",
    "- **`stop`**: secuencias de corte; si aparecen, la generación se detiene.\n",
    "- **`frequency_penalty` / `presence_penalty`**: penalizaciones para reducir repeticiones y fomentar aparición de nuevos términos.\n",
    "\n",
    "**Buenas prácticas:**\n",
    "- Controlar `temperature` (0.2–0.7) según la tarea (evaluación → bajo; lluvia de ideas → medio/alto).\n",
    "- Estructurar prompts y, cuando sea útil, **pedir JSON** para facilitar la automatización.\n",
    "- Evitar incluir datos personales o sensibles en prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873e7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial en la educación se refiere al uso de tecnologías avanzadas para personalizar y mejorar el proceso de enseñanza y aprendizaje, adaptando recursos y metodologías a las necesidades individuales de los estudiantes. Esto incluye herramientas como tutores virtuales, análisis de datos para evaluar el rendimiento académico y sistemas que facilitan la administración educativa.\n"
     ]
    }
   ],
   "source": [
    "# 5) Primera consulta: respuesta libre\n",
    "prompt = \"Explica en dos frases qué es la inteligencia artificial en la educación.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.7  # más creativo que 0.2, menos que 1.0\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e281741",
   "metadata": {},
   "source": [
    "### Comentarios sobre `temperature`\n",
    "- A `0.0–0.2`: respuestas casi siempre iguales (útil en **rubricas** o **explicaciones estándar**).\n",
    "- A `0.5–0.8`: más variación léxica/estilística (útil en **generación de materiales** o **brainstorming**).\n",
    "- A `>0.9`: creatividad alta pero riesgo de salidas menos precisas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f22c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"operation\": \"explanation\",\n",
      "  \"input\": \"¿Qué es aprendizaje supervisado?\",\n",
      "  \"output\": \"El aprendizaje supervisado es un tipo de aprendizaje automático donde un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza comúnmente en tareas como clasificación y regresión.\"\n",
      "}\n",
      "\n",
      "Valid JSON → {'operation': 'explanation', 'input': '¿Qué es aprendizaje supervisado?', 'output': 'El aprendizaje supervisado es un tipo de aprendizaje automático donde un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada entrada del conjunto de datos tiene una salida conocida, lo que permite al modelo aprender a predecir la salida a partir de nuevas entradas. Se utiliza comúnmente en tareas como clasificación y regresión.'}\n"
     ]
    }
   ],
   "source": [
    "# 6) Respuesta estructurada en JSON para automatización\n",
    "import json\n",
    "\n",
    "query = \"¿Qué es aprendizaje supervisado?\"\n",
    "schema_instruction = (\n",
    "    \"Responde en formato JSON con las claves: operation, input, output. \"\n",
    "    \"operation debe ser 'explanation'; input debe repetir la pregunta; output la explicación clara y breve.\"\n",
    ")\n",
    "response_json = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": schema_instruction},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ],\n",
    "    temperature=0.1,       # más determinista para formatos estructurados\n",
    "    max_tokens=300         # suficiente para una explicación breve\n",
    ")\n",
    "text = response_json.choices[0].message.content\n",
    "print(text)\n",
    "\n",
    "# (Opcional) intentar cargar como JSON si el modelo devolvió un objeto válido\n",
    "try:\n",
    "    data = json.loads(text)\n",
    "    print(\"\\nValid JSON →\", data)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\nLa salida no es JSON válido literal. Puedes parsearla manualmente o usar validadores/funciones JSON del proveedor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a1d82",
   "metadata": {},
   "source": [
    "## 7) Ejercicios propuestos\n",
    "1. Cambia `temperature` a 0.1, 0.5 y 0.9 y compara el estilo de las respuestas.\n",
    "2. Pide que el modelo responda **siempre en JSON** usando un `system` que lo exija. Verifica si cumple.\n",
    "3. Crea un prompt de tu área (p.ej., *programación*, *arquitectura*, *matemáticas*) que devuelva un JSON con `operation`, `input`, `steps` (lista) y `output`.\n",
    "4. Limita la longitud con `max_tokens` y observa si corta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eece738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompt\": \"Explica brevemente el principio de funcionamiento de un árbol de decisión.\",\n",
      "  \"respuesta\": \"Un árbol de decisión es un modelo de aprendizaje automático que utiliza una estructura jerárquica para tomar decisiones basadas en preguntas sobre las características de los datos. Cada nodo interno representa una prueba sobre un atributo, cada rama representa el resultado de la prueba y cada hoja representa una clase o resultado final, permitiendo clasificar o predecir resultados de manera intuitiva.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Plantilla reutilizable para el curso\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(ask_model(\n",
    "    \"Explica brevemente el principio de funcionamiento de un árbol de decisión.\",\n",
    "    temperature=0.4,\n",
    "    system=\"Responde en dos oraciones, tono docente y preciso. Siempre responde JSON con llaves promt y respuesta\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779287fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo para Cálculo de Energía Cinética ---\n",
      "{\n",
      "  \"operation\": \"Cálculo de Energía Cinética\",\n",
      "  \"input\": \"m=1500 kg, v=20 m/s\",\n",
      "  \"steps\": [\n",
      "    \"La fórmula de la energía cinética es: EK = (1/2) * m * v^2\",\n",
      "    \"Sustituyendo los valores: EK = (1/2) * 1500 kg * (20 m/s)^2\",\n",
      "    \"Calculando: EK = (1/2) * 1500 kg * 400 m^2/s^2\",\n",
      "    \"EK = 750 kg * 400 m^2/s^2\",\n",
      "    \"EK = 300000 kg*m^2/s^2\",\n",
      "    \"EK = 300000 J\"\n",
      "  ],\n",
      "  \"output\": \"300000 J\"\n",
      "}\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os # Importar para obtener la clave API, si se usa un entorno\n",
    "\n",
    "# ⚠️ ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Este es un ejemplo de cómo podrías inicializarlo (ajusta según tu configuración):\n",
    "# Si usas variables de entorno:\n",
    "# client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 💡 Para propósitos de este ejemplo, si no tienes la clave configurada \n",
    "# en el entorno, deberás inicializar 'client' o comentar esta línea si \n",
    "# la ejecución real ocurre en un entorno donde 'client' ya está definido.\n",
    "# Si estás ejecutando en un Jupyter/Colab con la clave ya definida, no es necesario.\n",
    "# Si la variable 'client' no está definida, el código fallará.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Función para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ⚠️ Esta función requiere que la variable 'client' esté definida globalmente\n",
    "    # y que la conexión a la API sea exitosa.\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except NameError:\n",
    "        return \"ERROR: La variable 'client' (OpenAI client) no está definida. Por favor, inicialízala.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (Física) ---\n",
    "\n",
    "# 🎯 PROMPT DE FÍSICA: Cálculo de Energía Cinética.\n",
    "prompt_fisica = \"Calcula la energía cinética de un coche que tiene una masa de 1500 kg y se mueve a una velocidad constante de 20 m/s. Formula la ley física utilizada.\"\n",
    "\n",
    "# 🤖 SYSTEM: Exige el formato JSON estricto.\n",
    "system_json_request = \"\"\"\n",
    "Tu única tarea es resolver el problema de Física.\n",
    "DEBES responder únicamente con un objeto JSON que contenga las siguientes claves:\n",
    "1. 'operation': La ley física utilizada o el objetivo del cálculo (e.g., 'Cálculo de Energía Cinética').\n",
    "2. 'input': Los datos iniciales proporcionados (e.g., 'm=1500 kg, v=20 m/s').\n",
    "3. 'steps': Una lista de cadenas, donde cada elemento detalla una etapa del cálculo o la formulación de la ley. Incluye la fórmula.\n",
    "4. 'output': El resultado final del cálculo con unidades (e.g., '300000 J').\n",
    "NO incluyas texto adicional fuera del objeto JSON.\n",
    "\"\"\"\n",
    "\n",
    "# --- Ejemplo de Uso y Ejecución ---\n",
    "\n",
    "print(\"--- Ejecutando Modelo para Cálculo de Energía Cinética ---\")\n",
    "print(ask_model(\n",
    "    prompt=prompt_fisica,\n",
    "    temperature=0.1, # Baja temperatura para consistencia y precisión\n",
    "    system=system_json_request\n",
    "))\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d332f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo para Cálculo de Elasticidad Precio de la Demanda ---\n",
      "{\n",
      "  \"operation\": \"Cálculo de Elasticidad Precio de la Demanda (EPD) utilizando el método del punto medio\",\n",
      "  \"input\": \"Pi=$10, Qi=100, Pf=$12, Qf=90\",\n",
      "  \"steps\": [\n",
      "    \"1. Calcular el cambio en la cantidad demandada: ΔQ = Qf - Qi = 90 - 100 = -10.\",\n",
      "    \"2. Calcular el cambio en el precio: ΔP = Pf - Pi = 12 - 10 = 2.\",\n",
      "    \"3. Calcular el promedio de la cantidad demandada: Qprom = (Qi + Qf) / 2 = (100 + 90) / 2 = 95.\",\n",
      "    \"4. Calcular el promedio del precio: Pprom = (Pi + Pf) / 2 = (10 + 12) / 2 = 11.\",\n",
      "    \"5. Aplicar la fórmula de la elasticidad: EPD = (ΔQ / Qprom) / (ΔP / Pprom) = (-10 / 95) / (2 / 11).\",\n",
      "    \"6. Calcular EPD: EPD = (-0.1053) / (0.1818) = -0.58.\",\n",
      "    \"7. Interpretación: La elasticidad precio de la demanda es -0.58, lo que indica que la demanda es inelástica.\"\n",
      "  ],\n",
      "  \"output\": {\n",
      "    \"value\": -0.58,\n",
      "    \"type\": \"inelástica\"\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json # Importar para una mejor manipulación de JSON\n",
    "\n",
    "# ⚠️ ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Si usas la librería de OpenAI, la variable 'client' debe estar definida \n",
    "# con tu clave API para que el código se ejecute correctamente.\n",
    "# Ejemplo (ajusta según tu entorno):\n",
    "# client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.3,\n",
    "              max_tokens: int = 400,\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Función para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ⚠️ Esta función requiere que la variable 'client' esté definida.\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Simulación de la respuesta para entornos sin la clave API configurada\n",
    "        # Si 'client' está definido y la clave es válida, se ejecutará la llamada real.\n",
    "        if 'client' not in globals() and 'client' not in locals():\n",
    "            print(\"AVISO: 'client' no está definido. Devolviendo una respuesta de ejemplo simulada.\")\n",
    "            return json.dumps({\n",
    "                \"operation\": \"Cálculo de Elasticidad Precio de la Demanda (EPD)\",\n",
    "                \"input\": \"Precio inicial ($10), Cantidad inicial (100 unidades), Precio final ($12), Cantidad final (90 unidades)\",\n",
    "                \"steps\": [\n",
    "                    \"Fórmula de la EPD (Método del Punto Medio): EPD = [(Qf - Qi) / ((Qf + Qi)/2)] / [(Pf - Pi) / ((Pf + Pi)/2)].\",\n",
    "                    \"Cálculo del cambio porcentual en la cantidad demandada: [(90 - 100) / ((90 + 100)/2)] = -10 / 95 ≈ -0.1053.\",\n",
    "                    \"Cálculo del cambio porcentual en el precio: [(12 - 10) / ((12 + 10)/2)] = 2 / 11 ≈ 0.1818.\",\n",
    "                    \"Cálculo de la EPD: -0.1053 / 0.1818 ≈ -0.579.\"\n",
    "                ],\n",
    "                \"output\": \"La Elasticidad Precio de la Demanda es aproximadamente -0.58. La demanda es inelástica porque el valor absoluto (|0.58|) es menor que 1.\"\n",
    "            }, indent=2)\n",
    "\n",
    "        # Llamada real a la API (si 'client' está definido)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except NameError:\n",
    "        return \"ERROR: La variable 'client' (OpenAI client) no está definida. Por favor, inicialízala.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (Economía) ---\n",
    "\n",
    "# 🎯 PROMPT DE ECONOMÍA: Cálculo de la EPD.\n",
    "prompt_economia = \"Calcula la Elasticidad Precio de la Demanda utilizando el método del punto medio si el precio de un bien sube de $10 a $12, causando que la cantidad demandada baje de 100 unidades a 90 unidades. Indica qué tipo de elasticidad es.\"\n",
    "\n",
    "# 🤖 SYSTEM: Exige el formato JSON estricto.\n",
    "system_json_request = \"\"\"\n",
    "Tu única tarea es resolver el problema de Economía y proporcionar la interpretación del resultado.\n",
    "DEBES responder únicamente con un objeto JSON que contenga las siguientes claves:\n",
    "1. 'operation': La ley económica utilizada (e.g., 'Cálculo de Elasticidad Precio de la Demanda (EPD)').\n",
    "2. 'input': Los datos iniciales proporcionados (e.g., 'Pi=$10, Qi=100, Pf=$12, Qf=90').\n",
    "3. 'steps': Una lista de cadenas, donde cada elemento detalla una etapa del cálculo, la fórmula y la interpretación.\n",
    "4. 'output': El resultado numérico final y el tipo de elasticidad.\n",
    "NO incluyas texto adicional, explicaciones ni encabezados fuera del objeto JSON.\n",
    "\"\"\"\n",
    "\n",
    "# --- Ejemplo de Uso y Ejecución ---\n",
    "\n",
    "print(\"--- Ejecutando Modelo para Cálculo de Elasticidad Precio de la Demanda ---\")\n",
    "print(ask_model(\n",
    "    prompt=prompt_economia,\n",
    "    temperature=0.1, # Muy baja temperatura para asegurar precisión numérica y formato\n",
    "    system=system_json_request\n",
    "))\n",
    "print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab2d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Ejecución con max_tokens BAJO (40 tokens) ---\n",
      "La fusión nuclear es un proceso físico en el cual dos núcleos atómicos ligeros se combinan para formar un núcleo más pesado, liberando una cantidad significativa de energía en el proceso\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- 2. Ejecución con max_tokens ALTO (400 tokens) ---\n",
      "La fusión nuclear es un proceso físico en el cual dos núcleos atómicos ligeros se combinan para formar un núcleo más pesado, liberando una cantidad significativa de energía en el proceso. Este fenómeno es el responsable de la producción de energía en las estrellas, incluida nuestra propia estrella, el Sol.\n",
      "\n",
      "### Proceso de Fusión Nuclear\n",
      "\n",
      "La fusión nuclear ocurre cuando los núcleos de dos átomos ligeros, como el hidrógeno, se acercan lo suficiente como para superar la repulsión electrostática que existe entre ellos debido a sus cargas positivas. Para que esto suceda, es necesario alcanzar condiciones extremas de temperatura y presión, que permiten que los núcleos se fusionen. En el caso del Sol, estas condiciones se logran en su núcleo, donde las temperaturas alcanzan aproximadamente 15 millones de grados Celsius y las presiones son inmensas.\n",
      "\n",
      "El proceso más común de fusión en el Sol es la fusión de isótopos de hidrógeno, como el deuterio y el tritio, que se combinan para formar helio, liberando energía en forma de radiación electromagnética y partículas. La reacción básica puede representarse de la siguiente manera:\n",
      "\n",
      "\\[ ^2H + ^3H \\rightarrow ^4He + n + energía \\]\n",
      "\n",
      "### Importancia de la Fusión Nuclear\n",
      "\n",
      "La fusión nuclear es considerada una fuente de energía potencialmente revolucionaria por varias razones:\n",
      "\n",
      "1. **Abundancia de Combustible**: Los isótopos de hidrógeno, como el deuterio, son abundantes en el agua de los océanos, lo que significa que la materia prima para la fusión es prácticamente inagotable.\n",
      "\n",
      "2. **Energía Limpia**: A diferencia de la fisión nuclear, que produce residuos radiactivos de larga vida, la fusión genera helio, un gas inerte y no contaminante. Además\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ⚠️ ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Este código asume que la variable 'client' está definida globalmente\n",
    "# y la conexión a la API está configurada.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400, # Valor por defecto más común\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Función para interactuar con un modelo de lenguaje y observar el efecto de max_tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Manejo básico de la ausencia de 'client' para simulación\n",
    "    try:\n",
    "        if 'client' not in globals() and 'client' not in locals():\n",
    "            raise NameError(\"La variable 'client' no está definida.\")\n",
    "            \n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens # 👈 El valor que se va a limitar\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except NameError:\n",
    "        # Respuesta simulada si 'client' no está definido\n",
    "        simulated_response = (\n",
    "            \"La fusión nuclear es el proceso mediante el cual dos núcleos atómicos ligeros se combinan para formar un núcleo más pesado. Este proceso libera una enorme cantidad de energía, mucho mayor que la fisión nuclear. Ocurre naturalmente en el Sol y las estrellas, donde las altas temperaturas y presiones superan la repulsión coulómbica de los núcleos. En la Tierra, se busca replicar esta reacción en reactores de confinamiento magnético (como los tokamaks) o inercial, utilizando isótopos de hidrógeno (deuterio y tritio) como combustible, para crear una fuente de energía limpia y prácticamente inagotable. Es el santo grial de la energía.\"\n",
    "        )\n",
    "        if max_tokens < len(simulated_response.split()):\n",
    "            return f\"SIMULACIÓN DE CORTE (max_tokens={max_tokens}): {simulated_response[:max_tokens*5]}...\" # Aprox. 5 caracteres por token en español\n",
    "        return simulated_response\n",
    "    except Exception as e:\n",
    "        return f\"Error al comunicarse con la API: {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso ---\n",
    "\n",
    "# 💬 PROMPT que pide una respuesta LAAAARGA para forzar el corte.\n",
    "prompt_largo = \"Explica detalladamente qué es la fusión nuclear, dónde ocurre, por qué es importante y los principales desafíos para su implementación en la Tierra. Necesito una explicación extensa.\"\n",
    "\n",
    "# 🤖 SYSTEM: Instrucción de tono simple.\n",
    "system_instruccion = \"Responde con un tono informativo y académico.\"\n",
    "\n",
    "\n",
    "# --- Ejemplo de Uso y Verificación ---\n",
    "\n",
    "print(\"--- 1. Ejecución con max_tokens BAJO (40 tokens) ---\")\n",
    "# El modelo debería detenerse abruptamente en medio de una frase.\n",
    "print(ask_model(\n",
    "    prompt=prompt_largo,\n",
    "    temperature=0.1, # Baja para que la respuesta sea coherente hasta el corte\n",
    "    max_tokens=40,   # 👈 Valor bajo para forzar el corte\n",
    "    system=system_instruccion\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"--- 2. Ejecución con max_tokens ALTO (400 tokens) ---\")\n",
    "# El modelo debería completar toda la explicación.\n",
    "print(ask_model(\n",
    "    prompt=prompt_largo,\n",
    "    temperature=0.1, \n",
    "    max_tokens=400, # 👈 Valor alto, suficiente para la respuesta\n",
    "    system=system_instruccion\n",
    "))\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b061c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Modelo con Límite de Tokens (max_tokens=40) ---\n",
      "Pregunta: Explica en detalle los principios fundamentales de la arquitectura Von Neumann y por qué se la conoce como el 'cuello de botella'.\n",
      "Límite de tokens establecido: 40\n",
      "\n",
      "### RESPUESTA CORTADA ###\n",
      "La arquitectura Von Neumann es un modelo de diseño de computadoras que fue propuesto por el matemático y físico John von Neumann en la década de 1940. Este modelo ha sido fundamental\n",
      "\n",
      "--- Observación ---\n",
      "Como puedes ver, la respuesta se interrumpe abruptamente (usualmente a mitad de una frase) porque el modelo alcanzó el límite de 40 tokens antes de terminar la explicación o de cerrar cualquier estructura (como un JSON o una frase completa).\n",
      "\n",
      "--- Ejecutando Modelo con Límite ALTO (max_tokens=400) para comparación ---\n",
      "\n",
      "### RESPUESTA COMPLETA (Simulada o Real) ###\n",
      "La arquitectura Von Neumann es un modelo de diseño de computadoras que fue propuesto por el matemático y físico John von Neumann en la década de 1940. Este modelo ha sido fundamental en el desarrollo de la computación moderna y se basa en una serie de principios que definen cómo se organizan y operan los componentes de un sistema informático. A continuación, se describen en detalle los principios fundamentales de la arquitectura Von Neumann y se explica por qué se le conoce como el \"cuello de botella\".\n",
      "\n",
      "### Principios Fundamentales de la Arquitectura Von Neumann\n",
      "\n",
      "1. **Unidad de Procesamiento Central (CPU)**:\n",
      "   - La CPU es el corazón del sistema y se encarga de ejecutar las instrucciones de los programas. Se compone de dos partes principales: la Unidad Aritmético-Lógica (ALU) y la Unidad de Control (CU).\n",
      "   - La ALU realiza operaciones aritméticas y lógicas, mientras que la CU gestiona el flujo de datos entre la CPU y otros componentes del sistema, decodificando las instrucciones y controlando su ejecución.\n",
      "\n",
      "2. **Memoria**:\n",
      "   - La arquitectura Von Neumann utiliza un único espacio de memoria para almacenar tanto los datos como las instrucciones del programa. Esto significa que la memoria es un recurso compartido, lo que permite que la CPU acceda a las instrucciones y a los datos de manera uniforme.\n",
      "   - La memoria se organiza en direcciones, y cada ubicación de memoria puede contener un dato o una instrucción. Esto permite que el sistema sea flexible y que las instrucciones se modifiquen durante la ejecución.\n",
      "\n",
      "3. **Bus de Datos**:\n",
      "   - La comunicación entre la CPU, la memoria y otros dispositivos periféricos se realiza a través de un bus de datos. Este bus es un conjunto de líneas de comunicación que transportan datos entre los diferentes componentes del sistema.\n",
      "   - El bus de datos es un canal compartido, lo que significa que solo\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os \n",
    "# ⚠️ ADVERTENCIA: Debes inicializar 'client' correctamente.\n",
    "# Si estás en un entorno donde 'client' no está definido, el código usará\n",
    "# la respuesta simulada para demostrar el corte.\n",
    "\n",
    "# --- Plantilla reutilizable para el curso ---\n",
    "def ask_model(prompt: str,\n",
    "              model: str = \"gpt-4o-mini\",\n",
    "              temperature: float = 0.1,\n",
    "              max_tokens: int = 400, # Valor por defecto\n",
    "              system: str | None = None):\n",
    "    \"\"\"\n",
    "    Función para interactuar con un modelo de lenguaje.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulación de la respuesta si 'client' no está definido,\n",
    "    # para demostrar el efecto del max_tokens.\n",
    "    if 'client' not in globals() and 'client' not in locals():\n",
    "        # Simulamos una respuesta que debería ser larga, pero se corta a 40 tokens.\n",
    "        simulated_response = {\n",
    "            \"prompt\": prompt,\n",
    "            \"respuesta_completa_esperada\": \"La arquitectura Von Neumann es un diseño fundamental que utiliza una única memoria para almacenar tanto las instrucciones del programa como los datos que serán procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella...\",\n",
    "            \"respuesta_simulada_cortada\": \"La arquitectura Von Neumann utiliza una única memoria para almacenar tanto las instrucciones del programa como los datos que serán procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella conocido como el 'cuello de botella de Von Neumann', que limita la velocidad de procesamiento al tener que acceder a una misma ruta para ambas tareas. Esta limitación afecta directamente el rendimiento...\",\n",
    "            \"respuesta_cortada_a_40_tokens\": \"La arquitectura Von Neumann utiliza una única memoria para almacenar tanto las instrucciones del programa como los datos que serán procesados. Esto simplifica la estructura de control, pero puede generar un cuello de botella conocido como el 'cuello de botella de Von Neumann', que limita la velocidad de procesamiento al tener que acceder a una misma ruta para\"\n",
    "        }\n",
    "        \n",
    "        # Devolvemos la respuesta cortada simulada para el ejercicio\n",
    "        # Nótese que el modelo real NO genera un JSON incompleto, sino que corta el texto.\n",
    "        if max_tokens <= 40:\n",
    "             # Devolvemos el texto cortado para demostrar el efecto\n",
    "             # Ignoramos temporalmente la exigencia de JSON para mostrar el corte puro.\n",
    "            return simulated_response[\"respuesta_cortada_a_40_tokens\"] + \"...\" \n",
    "        else:\n",
    "            return \"Simulación de respuesta larga sin corte.\"\n",
    "        \n",
    "    # Si 'client' está definido, se ejecuta la llamada real a la API\n",
    "    try:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "            \n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens # Aquí aplicamos el límite\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: La llamada real falló. {e}\"\n",
    "\n",
    "# --- Definiciones para el Ejemplo de Uso (Informática) ---\n",
    "\n",
    "# 🎯 PROMPT DE INFORMÁTICA: Pide una explicación detallada.\n",
    "prompt_informatica = \"Explica en detalle los principios fundamentales de la arquitectura Von Neumann y por qué se la conoce como el 'cuello de botella'.\"\n",
    "\n",
    "# 🤖 SYSTEM: Instrucción de tono (manteniendo el tono)\n",
    "system_request = \"Responde en un tono técnico y detallado. La respuesta debe ser lo más extensa posible para probar el límite de tokens.\"\n",
    "\n",
    "# --- Ejemplo de Uso con max_tokens BAJO (40) ---\n",
    "\n",
    "LIMITE_DE_TOKENS = 40\n",
    "\n",
    "print(\"--- Ejecutando Modelo con Límite de Tokens (max_tokens=40) ---\")\n",
    "print(f\"Pregunta: {prompt_informatica}\")\n",
    "print(f\"Límite de tokens establecido: {LIMITE_DE_TOKENS}\\n\")\n",
    "\n",
    "respuesta_cortada = ask_model(\n",
    "    prompt=prompt_informatica,\n",
    "    temperature=0.1,\n",
    "    max_tokens=LIMITE_DE_TOKENS, # 🚨 Aquí se aplica el límite bajo\n",
    "    system=system_request\n",
    ")\n",
    "\n",
    "print(\"### RESPUESTA CORTADA ###\")\n",
    "print(respuesta_cortada)\n",
    "print(\"\\n--- Observación ---\")\n",
    "print(\"Como puedes ver, la respuesta se interrumpe abruptamente (usualmente a mitad de una frase) porque el modelo alcanzó el límite de 40 tokens antes de terminar la explicación o de cerrar cualquier estructura (como un JSON o una frase completa).\")\n",
    "\n",
    "# --- Ejemplo de Uso con max_tokens ALTO (400) ---\n",
    "print(\"\\n--- Ejecutando Modelo con Límite ALTO (max_tokens=400) para comparación ---\")\n",
    "\n",
    "respuesta_completa = ask_model(\n",
    "    prompt=prompt_informatica,\n",
    "    temperature=0.1,\n",
    "    max_tokens=400,\n",
    "    system=system_request\n",
    ")\n",
    "\n",
    "print(\"\\n### RESPUESTA COMPLETA (Simulada o Real) ###\")\n",
    "print(respuesta_completa)\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fef7c",
   "metadata": {},
   "source": [
    "## 9) Solución de problemas comunes\n",
    "- **`openai.AuthenticationError` / `401`**: clave inválida o no cargada; revisa tu `.env` y reinicia el kernel.\n",
    "- **`Rate limit`**: excediste el número de solicitudes por minuto; espera unos segundos y reintenta.\n",
    "- **`model_not_found`**: el modelo no existe o no tienes acceso; cambia a uno disponible en tu cuenta.\n",
    "- **Salidas no JSON**: fija `temperature=0.0–0.3`, agrega instrucciones `system` estrictas y/o usa funciones nativas de validación JSON si el proveedor las ofrece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eab4f2",
   "metadata": {},
   "source": [
    "## 10) Próximos pasos\n",
    "En la siguiente sesión se verán **consultas avanzadas**, manejo de contexto y el punto de partida para construir un **asistente con RAG** (recuperación de conocimiento) usando materiales del curso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
